{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Music generation using Recurrent Neural Network.\n",
    "'''Let's import all the necesary libraries.'''\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os \n",
    "import time \n",
    "import functools\n",
    "from IPython import display as ipythondisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('irish.abc', 'https://raw.githubusercontent.com/aamini/introtodeeplearning_labs/2019/lab1/data/irish.abc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text 197618 characters\n"
     ]
    }
   ],
   "source": [
    "#Let's see the length of our dataset\n",
    "text=open(path_to_file).read()\n",
    "print(\"Length of text {} characters\".format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:1\n",
      "T:Alexander's\n",
      "Z: id:dc-hornpipe-1\n",
      "M:C|\n",
      "L:1/8\n",
      "K:D Major\n",
      "(3ABc|dAFA DFAd|fdcd FAdf|gfge fefd|(3efe (3dcB A2 (3ABc|!\n",
      "dAFA DFAd|fdcd FAdf|gfge fefd|(3efe dc d2:|!\n",
      "AG|FAdA FAdA|GBdB GBdB|Acec Acec|dfaf gecA|!\n",
      "FAdA FAdA|GBdB GBdB|Aceg fefd|(3efe dc d2:|!\n",
      "\n",
      "X\n"
     ]
    }
   ],
   "source": [
    "'''Lets have a look in our dataset we can not only see the lyrics but can gain meta information from it.'''\n",
    "print(text[:255])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83 unique characters\n"
     ]
    }
   ],
   "source": [
    "voc=sorted(set(text))\n",
    "print('{} unique characters'.format(len(voc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''We're trying to train a RNN model to learn patterns in ABC music, and then use this model to generate\n",
    "(i.e., predict) a new piece of music based on this learned information.given a character, or a sequence of characters, \n",
    "what is the most probable next character? We'll train the model to perform this task.'''\n",
    "#Let's start by vectorizing the text.\n",
    "char2id={u:i for i,u in enumerate(voc)}\n",
    "text_as_input=np.array([char2id[c]for c in text])\n",
    "idchar=np.array(voc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "'\\n':  0,\n",
      "...\n",
      "\n",
      "' ' :  1,\n",
      "...\n",
      "\n",
      "'!' :  2,\n",
      "...\n",
      "\n",
      "'\"' :  3,\n",
      "...\n",
      "\n",
      "'#' :  4,\n",
      "...\n",
      "\n",
      "\"'\" :  5,\n",
      "...\n",
      "\n",
      "'(' :  6,\n",
      "...\n",
      "\n",
      "')' :  7,\n",
      "...\n",
      "\n",
      "',' :  8,\n",
      "...\n",
      "\n",
      "'-' :  9,\n",
      "...\n",
      "\n",
      "'.' : 10,\n",
      "...\n",
      "\n",
      "'/' : 11,\n",
      "...\n",
      "\n",
      "'0' : 12,\n",
      "...\n",
      "\n",
      "'1' : 13,\n",
      "...\n",
      "\n",
      "'2' : 14,\n",
      "...\n",
      "\n",
      "'3' : 15,\n",
      "...\n",
      "\n",
      "'4' : 16,\n",
      "...\n",
      "\n",
      "'5' : 17,\n",
      "...\n",
      "\n",
      "'6' : 18,\n",
      "...\n",
      "\n",
      "'7' : 19,\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''Let's look at our numerical dataset now'''\n",
    "print(\"{\")\n",
    "for char,_ in zip(char2id,range(20)):\n",
    "    print('{:4s}:{:3d},'.format(repr(char),char2id[char]))\n",
    "    print(\"...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Our next step is to actually divide the text into example sequences that we'll use during training.\n",
    "Each input sequence that we feed into our RNN will contain seq_length characters from the text.\n",
    "We'll also need to define a target sequence for each input sequence,\n",
    "which will be used in training the RNN to predict the next character.\"'''\n",
    "seq_len=100\n",
    "ex_per_epoch=len(text)/seq_len\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create training examples / targets\n",
    "\n",
    "char_dataset=tf.data.Dataset.from_tensor_slices(text_as_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_len+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Next, we need to define the input and target texts for each sequence.\n",
    "\n",
    "I'll  define a function to do this, and then use the map method to apply a simple function to each batch.'''\n",
    "def split_input_target(chunk):\n",
    "    input_text=chunk[:-1]\n",
    "    target_text=chunk[1:]\n",
    "    return input_text,target_text\n",
    "dataset = sequences.map(split_input_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 49 ('X')\n",
      "  expected output: 22 (':')\n",
      "Step    1\n",
      "  input: 22 (':')\n",
      "  expected output: 13 ('1')\n",
      "Step    2\n",
      "  input: 13 ('1')\n",
      "  expected output: 0 ('\\n')\n",
      "Step    3\n",
      "  input: 0 ('\\n')\n",
      "  expected output: 45 ('T')\n",
      "Step    4\n",
      "  input: 45 ('T')\n",
      "  expected output: 22 (':')\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "  \n",
    "  for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "      print(\"Step {:4d}\".format(i))\n",
    "      print(\"  input: {} ({:s})\".format(input_idx, repr(idchar[input_idx])))\n",
    "      print(\"  expected output: {} ({:s})\".format(target_idx, repr(idchar[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((?, ?), (?, ?)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = ex_per_epoch//BATCH_SIZE\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.test.is_gpu_available():\n",
    "  LSTM = tf.keras.layers.CuDNNLSTM\n",
    "else:\n",
    "  LSTM = functools.partial(\n",
    "    tf.keras.layers.LSTM, recurrent_activation='sigmoid')\n",
    "\n",
    "LSTM = functools.partial(LSTM, \n",
    "  return_sequences=True, \n",
    "  recurrent_initializer='glorot_uniform',\n",
    "  stateful=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Let's define the mode'''\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    LSTM(rnn_units), # TODO: Define the dimensionality of the RNN\n",
    "    tf.keras.layers.Dense(vocab_size) # TODO: Define the dimensionality of the Dense layer\n",
    "  ])\n",
    "\n",
    "  return model            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
